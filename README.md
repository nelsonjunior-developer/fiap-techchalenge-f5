



## Vis√£o Geral e Objetivo de Neg√≥cio

### 1) Declara√ß√£o formal do objetivo
O objetivo deste projeto √© desenvolver um modelo de Machine Learning capaz de prever o risco de um estudante apresentar defasagem escolar no pr√≥ximo ano letivo (`t+1`), utilizando exclusivamente informa√ß√µes dispon√≠veis at√© o ano corrente (`t`). A previs√£o tem car√°ter preventivo e visa apoiar decis√µes educacionais da Associa√ß√£o Passos M√°gicos, priorizando alunos com maior risco.

### 2) Enquadramento do problema de Machine Learning
- Problema de `classifica√ß√£o bin√°ria` (risco vs. n√£o risco).
- Foco em `estimativa de risco futuro`, e n√£o em explica√ß√£o retrospectiva.
- Uso de dados futuros √© proibido para evitar `data leakage`.

### 3) Interpreta√ß√£o de neg√≥cio da defasagem escolar
No contexto da institui√ß√£o, defasagem escolar representa desalinhamento entre o n√≠vel educacional esperado e o n√≠vel efetivamente observado no estudante. Valores negativos indicam maior atraso em rela√ß√£o ao esperado e, portanto, maior risco educacional. O interesse de neg√≥cio est√° em antecipar essa condi√ß√£o no ano seguinte para permitir interven√ß√£o preventiva.
No dataset operacional, essa condi√ß√£o √© representada nos campos `Defas`/`Defasagem`, usados como refer√™ncia de risco educacional no recorte temporal.

### 4) Contexto de uso da previs√£o
- Usu√°rios potenciais: coordena√ß√£o pedag√≥gica, equipe psicopedag√≥gica e gest√£o educacional.
- Uso principal: prioriza√ß√£o de acompanhamento preventivo e aloca√ß√£o de suporte para alunos em risco.
- Decis√£o de risco: falsos negativos t√™m custo maior que falsos positivos, pois deixam de sinalizar alunos que precisariam de interven√ß√£o.
O modelo tem car√°ter preditivo e n√£o causal, sendo utilizado exclusivamente como ferramenta de apoio √† decis√£o humana.

### 5) Implica√ß√µes t√©cnicas assumidas nesta fase
- Horizonte temporal adotado: `t -> t+1`.
- Tipo de problema: `classifica√ß√£o bin√°ria`.
- M√©trica priorit√°ria: `Recall` (minimiza√ß√£o de falsos negativos).
- Sa√≠da esperada do modelo: `probabilidade de risco` (com posterior aplica√ß√£o de threshold operacional).
- Coorte temporal: pares v√°lidos consideram estudantes com `RA` presente em anos consecutivos (`t` e `t+1`).

### Escopo aberto do Datathon e motiva√ß√£o do problema escolhido
As orienta√ß√µes mais recentes do Datathon caracterizam o desafio como escopo aberto: a equipe pode propor abordagens distintas (como classifica√ß√£o, clusteriza√ß√£o ou solu√ß√µes com LLM), desde que a motiva√ß√£o do problema seja bem justificada no contexto de neg√≥cio e opera√ß√£o.

Decidimos manter o problema de classifica√ß√£o de risco de defasagem escolar em `t+1` porque ele √© diretamente acion√°vel para interven√ß√£o preventiva, prioriza√ß√£o pedag√≥gica e aloca√ß√£o de suporte. Essa escolha preserva alinhamento com a dor operacional da institui√ß√£o e com o uso pr√°tico esperado pelos stakeholders.

#### Justificativa: por que ‚Äúrisco de defasagem em t+1‚Äù √© acion√°vel

Escolhemos prever o risco de um estudante apresentar defasagem no pr√≥ximo ano (`t+1`) porque este √© um sinal diretamente operacionaliz√°vel para interven√ß√£o preventiva. Diferente de an√°lises puramente descritivas, o score de risco permite transformar dados em uma lista priorizada de alunos que devem receber aten√ß√£o antes que a defasagem se consolide.

Na pr√°tica, a probabilidade prevista pode ser usada para:
- Priorizar acompanhamento pedag√≥gico e psicopedag√≥gico (triagem) quando a capacidade de atendimento √© limitada.
- Direcionar refor√ßo e monitoria para grupos de maior risco, com a√ß√µes antes do fechamento do pr√≥ximo ciclo letivo.
- Padronizar crit√©rios de prioriza√ß√£o (reduzindo subjetividade) e registrar evid√™ncias de decis√£o para acompanhamento.

O custo de erro √© assim√©trico: falsos negativos (n√£o sinalizar um aluno que ficar√° defasado) t√™m impacto maior do que falsos positivos. Por isso, definimos `Recall` como m√©trica prim√°ria e operamos com threshold orientado a minimizar casos n√£o detectados, aceitando um aumento controlado de alertas.

Do ponto de vista de sistema, este problema tamb√©m √© adequado para produ√ß√£o porque:
- A gera√ß√£o de features usa apenas dados do ano corrente (`t`), evitando leakage.
- O r√≥tulo (`Defasagem_{t+1}`) chega com atraso, o que √© compat√≠vel com estrat√©gia de mensura√ß√£o offline (p√≥s-fato) e monitoramento online via drift/distribui√ß√£o de scores.
- O pipeline permite retreinamento peri√≥dico conforme novos dados anuais/semestrais entram, mantendo o modelo atualizado para mudan√ßas de popula√ß√£o e de processo.

#### Alternativas consideradas e por que n√£o adotamos como escopo principal

**1) Clusteriza√ß√£o (segmenta√ß√£o de perfis de alunos)**  
A clusteriza√ß√£o poderia identificar perfis como ‚Äúalto engajamento com dificuldade‚Äù, ‚Äúbaixo engajamento e queda persistente‚Äù ou ‚Äúbom desempenho e estabilidade‚Äù, apoiando a√ß√µes diferenciadas por grupo. N√£o adotamos como escopo principal porque exige valida√ß√£o qualitativa forte com especialistas (interpreta√ß√£o dos clusters), defini√ß√£o de m√©tricas de utilidade (n√£o h√° ‚Äúground truth‚Äù direto) e tende a aumentar o risco de subjetividade na entrega acad√™mica. A abordagem pode ser incorporada futuramente como camada complementar (ex.: cluster + risco) para orientar tipos de interven√ß√£o.

**2) Solu√ß√£o com LLM (assistente/relat√≥rios pedag√≥gicos)**  
Uma solu√ß√£o com LLM poderia gerar relat√≥rios individualizados e recomenda√ß√µes pedag√≥gicas a partir do hist√≥rico do aluno e apoiar professores na tomada de decis√£o. N√£o adotamos como escopo principal porque traz depend√™ncias fora da stack proposta, maior complexidade de governan√ßa (alucina√ß√µes, seguran√ßa, privacidade e auditoria), e requer valida√ß√£o operacional e crit√©rios de qualidade diferentes dos exigidos para um modelo supervisionado. O projeto atual mant√©m foco em previs√µes reproduz√≠veis, mensur√°veis e audit√°veis com dados tabulares.

**3) Classifica√ß√£o de risco de evas√£o (t -> t+1)**  
Prever evas√£o escolar seria altamente relevante para reten√ß√£o e planejamento de acompanhamento. N√£o adotamos como escopo principal porque, no dataset atual, a ‚Äúevas√£o‚Äù n√£o est√° rotulada de forma expl√≠cita e consistente: a aus√™ncia de `RA` em `t+1` pode refletir evas√£o, transfer√™ncia, mudan√ßa de cadastro ou outras causas (ambiguidade de r√≥tulo). Sem um contrato claro do processo, o target ficaria ruidoso e poderia induzir conclus√µes erradas. Ainda assim, as estat√≠sticas de coorte e interse√ß√£o por `RA` j√° implementadas s√£o base para explorar essa hip√≥tese com valida√ß√£o institucional.

**4) Prever melhora/piora de defasagem (delta de defasagem entre anos)**  
Modelar a varia√ß√£o (melhora/piora) da defasagem poderia apoiar identifica√ß√£o de trajet√≥rias e efic√°cia de interven√ß√µes. N√£o adotamos como escopo principal porque a formula√ß√£o depende de escolhas adicionais (regra do delta, discretiza√ß√£o, classes e interpreta√ß√£o) e pode ser mais sens√≠vel a ru√≠do e mudan√ßas de medi√ß√£o entre anos. Para esta entrega, preferimos um target mais direto e acion√°vel (‚Äúestar defasado em t+1‚Äù) com decis√£o de risco clara e prioridade em Recall. A previs√£o de delta pode ser explorada como extens√£o, reaproveitando o pareamento temporal j√° implementado.

O modelo continua com car√°ter preditivo de apoio √† decis√£o humana: n√£o √© causal nem prescritivo. O foco de engenharia deste projeto √© o sistema de ML em produ√ß√£o, incluindo entrada de alunos novos, valida√ß√£o por contrato, infer√™ncia, mensura√ß√£o em produ√ß√£o, monitoramento de drift, retreinamento e promo√ß√£o/rollback de vers√µes.

## Defini√ß√£o do Target

- Regra formal do target bin√°rio:
  - `y = 1` se `Defasagem_{t+1} < 0`
  - `y = 0` caso contr√°rio
- Comparador adotado: estritamente `< 0`.
- Recorte temporal oficial:
  - Treino: `X(2022) -> y(2023)`
  - Holdout final: `X(2023) -> y(2024)`
- Pol√≠tica para qualidade do target em `t+1`:
  - Tokens inv√°lidos (ex.: `#N/A`, `#DIV/0!`, `INCLUIR`) s√£o convertidos para `NaN` antes da defini√ß√£o de `y`.
  - Pares com target ausente/inv√°lido s√£o exclu√≠dos.
  - As contagens de exclus√£o por `missing` e `invalid` s√£o registradas em log.
- Regra de coorte por `RA`:
  - Apenas estudantes presentes em ambos os anos consecutivos (`t` e `t+1`) entram nos pares temporais.
- Regra anti-leakage:
  - `X` usa somente vari√°veis de `t`.
  - `y` √© calculado exclusivamente com `Defasagem` de `t+1`.
  - `RA` √© usado apenas como identificador/auditoria, nunca como feature.
  - O dataset de pares temporais implementa valida√ß√µes anti-leakage e falha caso colunas do ano `t+1` vazem para `X` (ex.: sufixos de merge).
- A m√©trica prim√°ria de sucesso √© Recall (minimizar falsos negativos). Como m√©tricas secund√°rias de acompanhamento e trade-off, reportamos PR-AUC (Average Precision), Precision, F1-score e ROC-AUC.

## An√°lise das Bases e Dicion√°rio

A an√°lise detalhada do dicion√°rio de dados e das bases `2022`, `2023` e `2024` est√° documentada em:

- [docs/analise_bases_e_dicionario.md](docs/analise_bases_e_dicionario.md)
- Regra de ingest√£o aplicada: `Defas` (2022) √© padronizada para `Defasagem` para manter schema √∫nico entre anos.

## Dados e Ingest√£o

- O arquivo XLSX do projeto cont√©m as abas `PEDE2022`, `PEDE2023` e `PEDE2024`.
- O caminho do arquivo pode ser configurado via `DATASET_PATH`.
- A leitura raw foi separada da padroniza√ß√£o:
  - `load_pede_workbook_raw` / `load_year_sheet_raw`: apenas leitura.
  - `load_pede_workbook` / `load_year_sheet`: wrappers com padroniza√ß√£o.
- A harmoniza√ß√£o de schema usa nomes can√¥nicos entre anos, incluindo:
  - `Defas -> Defasagem`
  - `Matem -> Mat`, `Portug -> Por`, `Ingl√™s -> Ing`
  - `Idade 22 -> Idade`
  - `Fase ideal/Fase Ideal -> Fase_Ideal`
  - `Nome/Nome Anonimizado -> Nome_Anon`
  - `Ano nasc/Data de Nasc -> Data_Nasc`
- Regras de fallback para colunas can√¥nicas derivadas:
  - `INDE` por ano:
    - 2022: `INDE 22`
    - 2023: `INDE 2023` -> `INDE 23` -> `INDE 22`
    - 2024: `INDE 2024` -> `INDE 23` -> `INDE 22`
  - `Pedra_Ano` por ano:
    - 2022: `Pedra 22` -> `Pedra 21` -> `Pedra 20`
    - 2023: `Pedra 2023` -> `Pedra 23` -> `Pedra 22`
    - 2024: `Pedra 2024` -> `Pedra 23` -> `Pedra 22`
- Duplicadas de planilha (`.1`, `.2`, ...) s√£o tratadas de forma determin√≠stica como `__dupN`, sem perda silenciosa.
- O crosswalk de equival√™ncia de colunas √© centralizado em `src/column_mapping.py` e documentado em `docs/column_mapping.md` / `docs/column_mapping.json`.
  - Aplica√ß√£o ocorre antes do alinhamento entre anos, com resolu√ß√£o de colis√µes por `combine_first` e auditoria em `metadata["column_mapping_report"]`.
  - Quando m√∫ltiplas colunas candidatas mapeiam para a mesma can√¥nica (ex.: `Defasagem` + `Defasagem__dup1`), elas s√£o combinadas (`combine_first`) e as colunas fonte s√£o descartadas para manter o schema can√¥nico.
- Nota sem√¢ntica importante:
  - `Ano nasc` e `Data de Nasc` n√£o s√£o semanticamente id√™nticos (ano vs data completa). Nesta fase harmonizamos header e aplicamos uma normaliza√ß√£o m√≠nima de conte√∫do para garantir tipo `datetime` e evitar quebra do pipeline. A interpreta√ß√£o sem√¢ntica fina (ex.: precis√£o de data vs apenas ano) continua sendo uma limita√ß√£o conhecida.
  - `Nome` e `Nome Anonimizado` s√£o harmonizados para `Nome_Anon` apenas para alinhamento de schema; isso n√£o garante anonimiza√ß√£o no dado de 2022.
- Padroniza√ß√£o de tipos ap√≥s harmoniza√ß√£o/alinhamento:
  - `Data_Nasc` √© padronizada para `datetime` com desambigua√ß√£o expl√≠cita:
    - valores num√©ricos em `1900..2100` s√£o interpretados como ano (`YYYY-01-01`)
    - demais num√©ricos s√£o interpretados como serial Excel (`origin=1899-12-30`)
  - `Idade` √© sanitizada para remover valores datetime (ex.: `1900-01-...`, que viram `NaN`) e convertida para `Int64` (nullable).
  - Nota t√©cnica (2023): quando `Idade` aparece como artefato de planilha no formato `1900-01-XX`, o pipeline interpreta `XX` como idade para recuperar o valor num√©rico. Essa recupera√ß√£o √© aplicada apenas quando o padr√£o indica claramente artefato de planilha e passa por valida√ß√£o de faixa plaus√≠vel (`3..30`).
  - Colunas num√©ricas usam dtypes nulos est√°veis (`Float64`/`Int64`) com coer√ß√£o robusta (`to_numeric(..., errors=\"coerce\")`), incluindo tratamento do token `INCLUIR`.
  - Colunas categ√≥ricas s√£o padronizadas para `string` com `strip`.
- Normaliza√ß√£o de categorias textuais:
  - `G√™nero`: `Menina/Menino` -> `Feminino/Masculino`
  - `Institui√ß√£o de ensino`: `Escola P√∫blica/Publica` -> `P√∫blica`; unifica√ß√£o de varia√ß√µes de capitaliza√ß√£o
  - `Pedra*` e `Pedra_Ano`: `Agata` -> `√Ågata`; `INCLUIR` -> `NA`
  - `Turma`: normaliza√ß√£o para `UPPERCASE`
  - `Fase` e `Fase_Ideal`: tratadas como categ√≥ricas textuais (n√£o num√©ricas) por instabilidade sem√¢ntica em 2024
  - Auditoria da normaliza√ß√£o em `artifacts/category_normalization_report.json`

## Valida√ß√£o de Consist√™ncia

- O validador checa automaticamente:
  - sanidade de `RA` (nulo, branco, duplicado)
  - missing por coluna com distin√ß√£o entre:
    - missing estrutural (coluna n√£o existia no ano e foi adicionada no alinhamento)
    - missing real (coluna existia no ano e est√° vazia)
  - coer√ß√£o/tipos inv√°lidos por coluna com base no relat√≥rio de tipagem
  - valida√ß√£o autom√°tica contra data contracts versionados em `docs/contracts/` (nome, tipo, missing e dom√≠nio por coluna)
  - respeito a `presence` (`original` vs `structural_optional`) e `enforcement` (`error`, `warning`, `info`)
- Execu√ß√£o padr√£o:

```bash
python -m src.validate
```

- Relat√≥rios gerados em:
  - `artifacts/data_quality_report.json` (sempre)
  - `artifacts/data_quality_report.md` (opcional)
- Modo de execu√ß√£o:
  - `strict=False` por padr√£o (gera relat√≥rio sem quebrar o fluxo)
  - use `--strict` para falhar o pipeline quando houver `ERROR`
  - use `--contracts-dir` para apontar um diret√≥rio customizado de contratos versionados

## Coorte e Interse√ß√£o por RA

- O projeto calcula estat√≠sticas de interse√ß√£o de `RA` entre anos para rastreabilidade do recorte temporal.
- O relat√≥rio agrega apenas contagens e percentuais (sem listas de identificadores).
- Execu√ß√£o:

```bash
python -m src.cohort_stats
```

- Artefatos gerados:
  - `artifacts/ra_intersections.json` (sempre)
  - `artifacts/ra_intersections.md` (opcional)

Resumo atual do dataset:

| Par | Interse√ß√£o | % do 1¬∫ ano | % do 2¬∫ ano | Uni√£o | Jaccard |
|---|---:|---:|---:|---:|---:|
| 2022 ‚à© 2023 | 600 | 69.77% | 59.17% | 1274 | 0.4710 |
| 2023 ‚à© 2024 | 765 | 75.44% | 66.18% | 1405 | 0.5445 |
| 2022 ‚à© 2024 | 472 | 54.88% | 40.83% | 1544 | 0.3057 |

## Data Contract

- O projeto mant√©m contratos de dados versionados por ano (`2022`, `2023`, `2024`) em:
  - `docs/contracts/data_contract_2022.json`
  - `docs/contracts/data_contract_2023.json`
  - `docs/contracts/data_contract_2024.json`
- Cada contrato define:
  - `dtype` esperado por coluna
  - `presence` da coluna no ano (`original` vs `structural_optional`)
  - `enforcement` por regra (`error`, `warning`, `info`)
  - dom√≠nios plaus√≠veis (`range`, `set`, `date_range`, `none`)
- O contrato tamb√©m marca campos sens√≠veis (`pii`) e metadados de linhagem (`contract_version`, `generated_at`, `dataset_basename`, `dataset_sha256`).
- Exporta√ß√£o dos contratos:

```bash
python -m src.contracts --export
```

## üìÅ Estrutura do Projeto

O reposit√≥rio √© organizado para separar claramente ingest√£o e tratamento de dados, treinamento do modelo, disponibiliza√ß√£o via API, monitoramento e testes, garantindo manutenibilidade, reprodutibilidade e facilidade de deploy.

```
fiap-techchalenge-f5/
‚îú‚îÄ‚îÄ README.md                     # documenta√ß√£o principal do projeto
‚îú‚îÄ‚îÄ .gitignore                    # regras de versionamento/artefatos ignorados
‚îú‚îÄ‚îÄ requirements.txt              # depend√™ncias de runtime
‚îú‚îÄ‚îÄ requirements-dev.txt          # depend√™ncias de desenvolvimento e testes
‚îú‚îÄ‚îÄ agents.md                     # conven√ß√µes operacionais para agentes LLM
‚îú‚îÄ‚îÄ app/                          # camada de aplica√ß√£o (API/serving)
‚îÇ   ‚îî‚îÄ‚îÄ model/                    # diret√≥rio de artefatos de modelo para infer√™ncia
‚îÇ       ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ artifacts/                    # artefatos gerados (modelos, metadados, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ dashboards/                   # dashboards de monitoramento/visualiza√ß√£o
‚îú‚îÄ‚îÄ docs/                         # documenta√ß√£o t√©cnica complementar
‚îÇ   ‚îú‚îÄ‚îÄ .gitkeep
‚îÇ   ‚îú‚îÄ‚îÄ analise_bases_e_dicionario.md  # an√°lise das bases e dicion√°rio de dados
‚îÇ   ‚îú‚îÄ‚îÄ column_mapping.md         # tabela de equival√™ncia de colunas entre anos
‚îÇ   ‚îú‚îÄ‚îÄ column_mapping.json       # espelho JSON do crosswalk de colunas
‚îÇ   ‚îî‚îÄ‚îÄ contracts/                # contratos versionados por ano
‚îÇ       ‚îú‚îÄ‚îÄ data_contract_2022.json
‚îÇ       ‚îú‚îÄ‚îÄ data_contract_2023.json
‚îÇ       ‚îî‚îÄ‚îÄ data_contract_2024.json
‚îú‚îÄ‚îÄ logs/                         # logs locais da aplica√ß√£o/pipeline
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ notebooks/                    # explora√ß√£o e experimentos
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ src/                          # c√≥digo-fonte do pipeline de dados e ML
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ categories.py             # normaliza√ß√£o textual de categorias e auditoria
‚îÇ   ‚îú‚îÄ‚îÄ column_mapping.py         # crosswalk e harmoniza√ß√£o de colunas equivalentes
‚îÇ   ‚îú‚îÄ‚îÄ cohort_stats.py           # estat√≠sticas de interse√ß√£o de RA por ano
‚îÇ   ‚îú‚îÄ‚îÄ config.py                 # constantes globais (ex.: RANDOM_STATE)
‚îÇ   ‚îú‚îÄ‚îÄ contracts.py              # defini√ß√£o/export de data contracts por ano
‚îÇ   ‚îú‚îÄ‚îÄ data.py                   # ingest√£o XLSX e gera√ß√£o de pares temporais
‚îÇ   ‚îú‚îÄ‚îÄ dtypes.py                 # padroniza√ß√£o de tipos e auditoria de coer√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ features.py               # sele√ß√£o de features e split num/cat/datetime
‚îÇ   ‚îú‚îÄ‚îÄ imputation.py             # plano de imputa√ß√£o de missing para treino/infer√™ncia
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py          # ColumnTransformer com imputa√ß√£o + one-hot + escalonamento num√©rico opcional
‚îÇ   ‚îú‚îÄ‚îÄ contract_validate.py      # valida√ß√£o autom√°tica dos data contracts
‚îÇ   ‚îú‚îÄ‚îÄ schema.py                 # harmoniza√ß√£o/alinhamento de schema entre anos
‚îÇ   ‚îú‚îÄ‚îÄ utils.py                  # utilit√°rios compartilhados (ex.: logging)
‚îÇ   ‚îî‚îÄ‚îÄ validate.py               # valida√ß√£o de consist√™ncia e gera√ß√£o de relat√≥rios
‚îî‚îÄ‚îÄ tests/                        # su√≠te de testes automatizados
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ test_categories.py        # testes de normaliza√ß√£o de categorias
    ‚îú‚îÄ‚îÄ test_cohort_stats.py      # testes de interse√ß√£o por RA e privacidade
    ‚îú‚îÄ‚îÄ conftest.py               # configura√ß√£o compartilhada dos testes
    ‚îú‚îÄ‚îÄ test_column_mapping.py    # testes do crosswalk e harmoniza√ß√£o de equival√™ncias
    ‚îú‚îÄ‚îÄ test_config.py            # testes de configura√ß√£o global
    ‚îú‚îÄ‚îÄ test_contract_validate.py # testes da valida√ß√£o autom√°tica de contratos
    ‚îú‚îÄ‚îÄ test_contracts.py         # testes dos contratos de dados por ano
    ‚îú‚îÄ‚îÄ test_data.py              # testes de ingest√£o e pares temporais
    ‚îú‚îÄ‚îÄ test_dtypes.py            # testes da padroniza√ß√£o de tipos
    ‚îú‚îÄ‚îÄ test_features.py          # testes da sele√ß√£o/split de features
    ‚îú‚îÄ‚îÄ test_imputation.py        # testes da pol√≠tica e plano de imputa√ß√£o
    ‚îú‚îÄ‚îÄ test_inference_reusability.py # testes do contrato de entrada e reuso do pr√©-processamento na infer√™ncia
    ‚îú‚îÄ‚îÄ test_logging.py           # testes de logging centralizado
    ‚îú‚îÄ‚îÄ test_preprocessing_bundle.py # testes de integra√ß√£o do bundle (raw -> engineered -> preprocessor)
    ‚îú‚îÄ‚îÄ test_preprocessing.py     # testes do ColumnTransformer e OneHotEncoder
    ‚îú‚îÄ‚îÄ test_schema.py            # testes de harmoniza√ß√£o/alinhamento de schema
    ‚îî‚îÄ‚îÄ test_validate.py          # testes do validador de consist√™ncia
```

## Ambiente Local (.venv)

### macOS / Linux

1) Criar e ativar ambiente virtual

```bash
python3 -m venv .venv
source .venv/bin/activate
```

2) Instalar depend√™ncias

```bash
python -m pip install --upgrade pip
pip install -r requirements.txt
```

Reprodutibilidade: `RANDOM_STATE = 42` √© usado globalmente no projeto.

### Execu√ß√£o de Testes

1) Instalar depend√™ncias de desenvolvimento

```bash
source .venv/bin/activate
pip install -r requirements-dev.txt
```

2) Rodar su√≠te de testes

```bash
pytest -q
```

3) Rodar testes com cobertura (comando oficial do projeto)

```bash
pytest --cov=src --cov-report=term-missing --cov-fail-under=80
```

Observa√ß√£o: mantenha este comando de cobertura sempre documentado no `README.md` para padronizar valida√ß√£o local e evid√™ncia t√©cnica da entrega.

## Logging

- Logging b√°sico centralizado em `src/utils.py` com `setup_logging()` e `get_logger()`.
- N√≠vel padr√£o: `INFO`. Para ajustar em runtime:
  - `LOG_LEVEL=DEBUG` (ou `INFO`, `WARNING`, `ERROR`)
  - Valores inv√°lidos de `LOG_LEVEL` fazem fallback para `INFO` (com warning em log).
- Sa√≠da padr√£o: `stdout`.
- Opcional: habilitar arquivo em `logs/app.log` com:
  - `LOG_TO_FILE=1`
- Idempot√™ncia: `setup_logging()` pode ser chamado m√∫ltiplas vezes sem duplicar handlers/logs.
- Privacidade operacional:
  - N√£o logar `RA`, listas de identificadores, payloads completos ou dados pessoais.
  - Logar apenas m√©tricas agregadas e contadores operacionais.

## Separa√ß√£o de Features (Fase 4)

- A sele√ß√£o de features exclui PII por padr√£o a partir de `PII_COLUMNS` do contrato (`Nome_Anon` e `Avaliador*`), sem depender de listas duplicadas.
- As features s√£o separadas por dtypes em tr√™s grupos: `numeric`, `categorical` e `datetime` (com `Data_Nasc` tratada explicitamente como datetime nesta etapa).
- O split pode ser auditado no pareamento temporal:
  - `make_temporal_pairs(..., persist_feature_split=True)` gera `artifacts/feature_split_report.json`.
  - por padr√£o (`persist_feature_split=False`), n√£o h√° side effect de escrita em disco.

## Imputa√ß√£o de Missing (Fase 4)

- A pol√≠tica de imputa√ß√£o √© definida como plano audit√°vel em `src/imputation.py`, para uso dentro de `Pipeline/ColumnTransformer` na etapa de treino.
- Estrat√©gias padr√£o:
  - num√©ricas: `median` + `add_indicator=True`
  - categ√≥ricas: `most_frequent` + `add_indicator=True`
  - datetime (`Data_Nasc`): n√£o imputado nesta etapa; permanece em `datetime_cols_excluded` para tratamento posterior de feature engineering.
- Colunas 100% missing no recorte real de treino (`2022->2023`) s√£o removidas do conjunto de imputa√ß√£o quando `drop_all_missing_columns=True`:
  - `Ativo/ Inativo`, `Ativo/ Inativo__dup1`, `Destaque IPV__dup1`, `Escola`, `INDE 2023`, `INDE 2024`, `INDE 23`, `IPP`, `Pedra 2023`, `Pedra 2024`, `Pedra 23`, `Rec Psicologia`
- Evid√™ncia local:
  - `artifacts/imputation_plan.json` (gerado por `persist_imputation_plan(...)`).

## Codifica√ß√£o Categ√≥rica (Fase 4)

- A codifica√ß√£o categ√≥rica √© feita com `OneHotEncoder(handle_unknown="ignore")` para tolerar categorias novas em produ√ß√£o sem quebrar infer√™ncia.
- O bloco categ√≥rico √© aplicado ap√≥s imputa√ß√£o (`SimpleImputer(strategy="most_frequent", add_indicator=True)`), dentro de `ColumnTransformer` em `src/preprocessing.py`.
- O bloco num√©rico usa `SimpleImputer(strategy="median", add_indicator=True)`.
- `Fase` e `Fase_Ideal` permanecem categ√≥ricas nesta etapa.
- `Data_Nasc` (datetime) n√£o entra na codifica√ß√£o nesta fase; fica para feature engineering posterior.

## Escalonamento Num√©rico (Fase 4)

- O pr√©-processador agora suporta escalonamento num√©rico configur√°vel em `src/preprocessing.py`.
- Regra adotada:
  - baseline linear (`Logistic Regression`): usar `numeric_scaler="standard"` (preset `DEFAULT_SCALER_FOR_LINEAR`);
  - modelos de √°rvore (ex.: `HistGradientBoosting`): usar `numeric_scaler="none"` (preset `DEFAULT_SCALER_FOR_TREE`).
- O escalonador pode ser configurado entre `standard`, `robust` e `none`, com valida√ß√£o expl√≠cita de par√¢metro.

## Reuso do Pr√©-processamento na Infer√™ncia (Fase 4)

- O contrato de entrada para infer√™ncia √© exposto por `get_expected_raw_feature_columns()` e deriva diretamente de `get_feature_columns_for_model()` (fonte √∫nica de verdade).
- `validate_inference_frame(...)` valida:
  - tipo de entrada (`pandas.DataFrame`);
  - colunas m√≠nimas esperadas (falha com erro claro para colunas faltantes);
  - colunas extras s√£o permitidas por padr√£o (registradas apenas por nome/contagem).
- `build_preprocessing_bundle(...)` entrega um bundle reutiliz√°vel para treino/API contendo:
  - `expected_raw_cols` (contrato da API),
  - `expected_model_cols` (raw + engineered),
  - `excluded_cols`, `numeric_scaler` e `preprocessor`,
  - `transform_raw_to_model_frame(...)` para aplicar feature engineering internamente antes do `ColumnTransformer`.
- O contrato da API valida somente colunas raw; as features derivadas s√£o detalhe interno do pipeline.

## Feature Engineering (Fase 4)

- Features derivadas simples e anti-leakage (somente dados de `t`) s√£o criadas em `src/features.py` por `add_engineered_features(...)`.
- Num√©ricas:
  - `avg_grades`, `min_grade`, `max_grade`, `grade_std`, `missing_grades_count`
  - `missing_indicators_count`
  - `defasagem_abs`, `defasagem_neg_flag`, `age_is_missing_flag`
- Categ√≥rica opcional:
  - `age_bucket` (`07_10`, `11_14`, `15_18`, `19_plus`)
- A engenharia √© opt-in no bundle (`enable_feature_engineering=True/False`) e pode incluir/excluir `age_bucket` (`enable_age_bucket`).

## Checklist do Projeto - Datathon Machine Learning Engineering

Este checklist foi elaborado considerando explicitamente as inconsist√™ncias reais do dataset fornecido (schemas distintos entre anos, colunas duplicadas, valores inv√°lidos, mudan√ßas sem√¢nticas de campos e interse√ß√£o parcial de estudantes entre per√≠odos). As etapas descritas adotam pr√°ticas de Data Engineering e MLOps para garantir robustez, reprodutibilidade e validade estat√≠stica do modelo em produ√ß√£o.

Status: `TODO` | `DOING` | `DONE` | `BLOCKED`

Progresso geral (barra visual):
`[üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú]`

`52 de 110 tarefas conclu√≠das (47.3%)`

| Fase | Progresso |
|---|---|
| Fase 1 - Entendimento do Problema e Target | 13/13 |
| Fase 2 - Organiza√ß√£o do Projeto e Ambiente | 7/7 |
| Fase 3 - Ingest√£o, Qualidade e Governan√ßa de Dados | 14/14 |
| Fase 4 - Pr√©-processamento e Engenharia de Features | 6/10 |
| Fase 5 - Pipeline, Treinamento e Avalia√ß√£o | 0/17 |
| Fase 6 - Artefatos, API e Deploy | 0/15 |
| Fase 7 - Testes, Monitoramento e Dashboard | 2/13 |
| Fase 8 - Documenta√ß√£o e Entrega Final | 10/21 |
| Total | 52/110 |

### Fase 1 - Entendimento do Problema e Target [13/13]
- [x] Compreender o objetivo de neg√≥cio: prever o risco de defasagem escolar (t+1)
- [x] Estudar o dicion√°rio de dados e as bases de 2022, 2023 e 2024
- [x] Padronizar a coluna de defasagem (`Defas` -> `Defasagem`)
- [x] Definir a formula√ß√£o do target bin√°rio
- [x] Definir m√©trica prim√°ria de sucesso (`Recall`) e m√©tricas secund√°rias (`PR-AUC`, `Precision`, `F1`, `ROC-AUC`) j√° na fase de desenho
- [x] Definir `y = 1` se `Defasagem_{t+1} < 0`
- [x] Definir `y = 0` caso contr√°rio
- [x] Definir a estrat√©gia de pares temporais
- [x] Definir treino: `X(2022) -> y(2023)`
- [x] Definir holdout final: `X(2023) -> y(2024)`
- [x] Garantir que `RA` seja usado apenas como ID, nunca como feature
- [x] Justificar o problema escolhido no contexto de escopo aberto (por que risco de defasagem `t+1` √© acion√°vel e √∫til)
- [x] Documentar alternativas consideradas (clusteriza√ß√£o, LLM, evas√£o, prever melhora/piora) e por que n√£o adotamos (1 par√°grafo cada)

### Fase 2 - Organiza√ß√£o do Projeto e Ambiente [7/7]
- [x] Configurar `.gitignore` inicial (ignorar `agents.md`, `dataset/` e `.DS_Store`)
- [x] Expandir `.gitignore` com padr√µes essenciais de Python/MLOps (cache, venv, cobertura, builds, logs e segredos locais)
- [x] Criar estrutura de diret√≥rios do projeto
- [x] Criar `requirements.txt` com depend√™ncias m√≠nimas
- [x] Fixar vers√µes das depend√™ncias para garantir reprodutibilidade do ambiente de execu√ß√£o
- [x] Definir `random_state` global para reprodutibilidade
- [x] Configurar logging b√°sico do projeto

### Fase 3 - Ingest√£o, Qualidade e Governan√ßa de Dados [14/14]
Camadas conceituais desta fase:
- Camada A - Pr√©-ingest√£o e Ingest√£o: contrato de dados, mapeamento de colunas equivalentes, tratamento de headers duplicados, normaliza√ß√£o de valores inv√°lidos, padroniza√ß√£o de datas e normaliza√ß√£o sem√¢ntica.
- Camada B - Governan√ßa e Valida√ß√£o Cont√≠nua: coorte temporal por `RA`, valida√ß√µes de shift, versionamento de dataset e privacidade operacional.

Nota de coorte temporal:
> A constru√ß√£o dos pares temporais considera apenas estudantes presentes em ambos os anos consecutivos (`t` e `t+1`), evitando vi√©s por evas√£o ou entrada tardia e garantindo consist√™ncia estat√≠stica na defini√ß√£o do target.

- [x] Implementar leitura do arquivo XLSX
- [x] Tratar diferen√ßas de colunas entre os anos
- [x] Padronizar nomes e tipos de dados
- [x] Criar fun√ß√£o de gera√ß√£o dos pares temporais (`t -> t+1`)
- [x] Validar consist√™ncia dos dados (missing, tipos inv√°lidos)
- [x] Definir um data contract por ano (nome, tipo e dom√≠nio esperado por coluna)
- [x] Implementar valida√ß√£o autom√°tica do data contract (asserts de nome, tipo e dom√≠nio por coluna)
- [x] Criar tabela de mapeamento entre colunas equivalentes (`Matem/Portug/Ingl√™s` <-> `Mat/Por/Ing`; `Defas` <-> `Defasagem`)
- [x] Tratar headers duplicados na ingest√£o com regra determin√≠stica
- [x] Normalizar valores inv√°lidos em campos num√©ricos (ex.: `#N/A`, `#DIV/0!`, `INCLUIR`)
- [x] Padronizar datas de nascimento para formato √∫nico
- [x] Normalizar categorias textuais entre anos (`Menina/Menino` <-> `Feminino/Masculino`; `Escola P√∫blica` <-> `P√∫blica`)
- [x] Definir regra formal de coorte temporal por `RA` (entradas, sa√≠das e interse√ß√µes por ano)
- [x] Gerar e registrar estat√≠sticas de interse√ß√£o por `RA` entre anos (contagem absoluta e percentual)

### Fase 4 - Pr√©-processamento e Engenharia de Features [6/10]
- [x] Separar features num√©ricas e categ√≥ricas
- [x] Tratar valores ausentes (imputa√ß√£o)
- [x] Codificar vari√°veis categ√≥ricas (`OneHotEncoder` ou similar)
- [x] Escalonar vari√°veis num√©ricas (se necess√°rio)
- [x] Garantir que o pr√©-processamento seja reutiliz√°vel na infer√™ncia
- [x] Criar novas features relevantes (se aplic√°vel)
- [ ] Implementar checagem expl√≠cita de data leakage (lista negra de colunas futuras + asserts temporais)
- [ ] Remover colunas irrelevantes ou com leakage
- [ ] Garantir que nenhuma feature use informa√ß√£o futura
- [ ] Documentar as principais decis√µes de feature engineering

### Fase 5 - Pipeline, Treinamento e Avalia√ß√£o [0/17]
Nota de shift temporal:
> Antes do treinamento final, √© realizada uma an√°lise de shift temporal do target e das features, uma vez que a preval√™ncia da classe positiva varia significativamente entre os per√≠odos analisados (aprox. `61%` para `40%`).

- [ ] Criar `ColumnTransformer` para pr√©-processamento
- [ ] Encapsular tudo em uma `Pipeline` do scikit-learn
- [ ] Garantir consist√™ncia treino vs infer√™ncia
- [ ] Validar que a pipeline aceita dados crus da API
- [ ] Treinar modelo baseline (`Logistic Regression`)
- [ ] Treinar modelo n√£o-linear (ex.: `HistGradientBoosting`)
- [ ] Usar apenas dados de treino (`2022 -> 2023`)
- [ ] (Opcional) Valida√ß√£o interna (CV estratificada)
- [ ] Definir estrat√©gia expl√≠cita para desbalanceamento de classes (`class_weight`, ajuste de threshold ou decis√£o justificada de n√£o tratar)
- [ ] Comparar modelos com foco em Recall e PR-AUC
- [ ] Avaliar desempenho no holdout temporal (`2023 -> 2024`)
- [ ] Calcular m√©tricas: Recall, Precision, F1-score, ROC-AUC, PR-AUC
- [ ] Gerar matriz de confus√£o
- [ ] Definir threshold operacional focado em Recall
- [ ] Definir crit√©rio objetivo formal de sele√ß√£o do modelo final (ex.: maior Recall com PR-AUC acima de limiar m√≠nimo)
- [ ] Justificar escolha do modelo final
- [ ] Incluir valida√ß√£o de shift temporal do target e das features antes do treinamento final

### Fase 6 - Artefatos, API e Deploy [0/15]
- [ ] Salvar pipeline completa em `model.joblib`
- [ ] Criar `metadata.json` com modelo, m√©tricas, threshold, features esperadas, data do treino e vers√µes das bibliotecas
- [ ] Salvar dados de refer√™ncia para monitoramento de drift
- [ ] Versionar dataset de treino/valida√ß√£o (`hash/checksum` + vers√£o usada no experimento)
- [ ] Definir schema formal de sa√≠da do modelo/API (probabilidade, classe prevista, threshold aplicado e vers√£o do modelo)
- [ ] Criar aplica√ß√£o FastAPI
- [ ] Implementar endpoint `POST /predict`
- [ ] Implementar `GET /health` e `GET /version`
- [ ] Validar entradas com Pydantic
- [ ] Garantir carregamento do modelo salvo
- [ ] Criar Dockerfile enxuto baseado em `python:slim`
- [ ] Documentar comandos de build e run no README
- [ ] Implementar versionamento de modelos local (ex.: `artifacts/models/<model_version>/` com `model.joblib` + `metadata.json`)
- [ ] Definir estrat√©gia de promo√ß√£o de modelo (staging -> prod local) com crit√©rio objetivo (Recall/PR-AUC/threshold)
- [ ] Documentar procedimento de atualiza√ß√£o do modelo na API (troca de vers√£o e rollback local)

### Fase 7 - Testes, Monitoramento e Dashboard [2/13]
- [x] Criar testes unit√°rios e de integra√ß√£o com pytest
- [x] Garantir cobertura m√≠nima de 80% com `pytest-cov`
- [ ] Adicionar CI automatizada (rodar `pytest`, coverage, `python -m src.validate` e `python -m src.cohort_stats`)
- [ ] Definir comportamento para alunos novos (sem hist√≥rico): valida√ß√£o de contrato, imputa√ß√£o/valores default e logging da taxa de campos ausentes
- [ ] Definir estrat√©gia de mensura√ß√£o em produ√ß√£o com "ground truth delay" (m√©tricas online vs m√©tricas offline quando o r√≥tulo chega)
- [ ] Implementar logging agregado de infer√™ncia (distribui√ß√£o de scores, taxa de positivos por threshold, taxa de erro de valida√ß√£o) sem PII
- [ ] Implementar rotina de avalia√ß√£o p√≥s-fato (quando labels `t+1` chegam) para medir Recall/PR-AUC em produ√ß√£o (mesmo que simulado)
- [ ] Definir pol√≠tica de reten√ß√£o/limpeza de logs e artefatos locais (script simples + documenta√ß√£o)
- [ ] Implementar teste de n√£o-regress√£o do modelo com limiares m√≠nimos de m√©tricas (ex.: Recall e/ou PR-AUC)
- [ ] Configurar logging estruturado
- [ ] Aplicar pol√≠tica de privacidade operacional (n√£o logar identificadores sens√≠veis como `RA` em API e monitoramento)
- [ ] Implementar relat√≥rio de drift com Evidently
- [ ] Criar aplica√ß√£o Streamlit para visualiza√ß√£o do relat√≥rio de drift

### Fase 8 - Documenta√ß√£o e Entrega Final [10/21]
- [x] Documentar vis√£o geral do problema e objetivo
- [ ] Documentar stack tecnol√≥gica
- [ ] Adicionar versionamento/changelog dos contratos (`docs/contracts`)
- [x] Documentar estrutura do projeto
- [ ] Documentar etapas do pipeline de Machine Learning
- [ ] Documentar ciclo de vida em produ√ß√£o: entrada de alunos novos, valida√ß√£o de contrato, infer√™ncia, logging, drift, retreino, promo√ß√£o/rollback
- [ ] Documentar explicitamente contratos em produ√ß√£o (data contracts + contrato de payload da API + contrato de sa√≠da)
- [ ] Documentar estrat√©gia de retreino (gatilhos por tempo e/ou por drift, e como executar)
- [ ] Documentar limita√ß√µes conhecidas do modelo e riscos assumidos
- [ ] Documentar exemplos de chamadas √† API
- [x] Documentar setup de ambiente local com `.venv` e instala√ß√£o de depend√™ncias
- [ ] Publicar c√≥digo organizado no GitHub
- [ ] Disponibilizar API acess√≠vel localmente
- [ ] Gravar v√≠deo gerencial (<= 5 minutos) explicando a solu√ß√£o
- [x] Criar `agents.md` com conven√ß√µes operacionais para agentes LLM
- [x] Adicionar barra de progresso geral visual (`[üü©‚¨ú...]`) no checklist
- [x] Atualizar `agents.md` com regra expl√≠cita de manuten√ß√£o da barra visual e da contagem geral
- [x] Incorporar recomenda√ß√µes da revis√£o t√©cnica do checklist (gaps de maturidade por fase)
- [x] Refinar reda√ß√£o do objetivo para "apresentar defasagem no t+1" (evita ambiguidade de transi√ß√£o vs estado)
- [x] Refinar vis√£o geral com v√≠nculo expl√≠cito a `Defas/Defasagem` e regra de coorte por `RA`
- [x] Adicionar men√ß√£o expl√≠cita de n√£o-causalidade do modelo na se√ß√£o de contexto de uso

<details>
<summary>Notas de uso do checklist</summary>

- Atualize os contadores de progresso de cada fase ao concluir tarefas.
- Atualize a barra visual de progresso geral (`[üü©‚¨ú...]`) com base na porcentagem conclu√≠da.
- Regra da barra: 40 blocos (`1 bloco = 2,5%`), com arredondamento para baixo.
- Marque uma tarefa como `DOING` no texto do item quando estiver em andamento.
- Promova para `DONE` apenas ap√≥s evid√™ncia (teste, artefato, log ou documenta√ß√£o).
- Use `BLOCKED` quando depender de decis√£o, dado externo ou ajuste de escopo.

</details>
